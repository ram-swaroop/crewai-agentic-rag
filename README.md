# crewai-agentic-rag
An advanced agentic RAG system built using CrewAI, integrating LangChain with Gemini 2.0 Flash and Ollama based Qwen2.5:3B models. It performs intelligent routing, vector-based document search (RAG), web search fallback, hallucination grading, and final answer synthesisâ€”all coordinated through autonomous agents.


# Project Structure
```
crewai-agentic-rag/

â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ router_agent.py
â”‚   â”œâ”€â”€ retriever_agent.py
â”‚   â”œâ”€â”€ grader_agent.py
â”‚   â”œâ”€â”€ hallucination_grader.py
â”‚   â”œâ”€â”€ answer_grader.py
â”‚
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ router_task.py
â”‚   â”œâ”€â”€ retriever_task.py
â”‚   â”œâ”€â”€ grader_task.py
â”‚   â”œâ”€â”€ hallucination_task.py
â”‚   â”œâ”€â”€ answer_task.py
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rag_tool.py
â”‚   â”œâ”€â”€ web_search_tool.py
â”‚   â”œâ”€â”€ router_tool.py
â”‚
â”œâ”€â”€ crew/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ crew_setup.py
â”‚
â”œâ”€â”€ knowledge/
â”‚   â””â”€â”€ add-your-pdf-here/
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_rag_tool.py
â”‚   â”œâ”€â”€ test_router_task_agent.py
â”‚   â””â”€â”€ test_web_search_tool.py
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```


### ğŸš€ Features
- **LLM-Driven Routing**: Directs questions to appropriate retrieval method.
- **PDF (RAG) and Web Search**: Combines structured knowledge with real-time web intelligence.
- **Answer Verification Pipeline**: Includes relevance grading and hallucination detection.
- **LLMs Used**:
  - Qwen2.5:3B via Ollama (using Local CPU)
  - Gemini 2.0 Flash (via LangChain)

---

## ğŸš€ Quick Start

### â–¶ï¸ Run with Python

```bash
# 1. Install dependencies (requires Python 3.12.8)
pip install -r requirements.txt

# 2. Run analysis
python main.py

# Examples
python main.py -q "How does DSPy differ from traditional methods of prompting language models in terms of modularity and optimization?" # [should use knowledge base]
python main.py -q "How does DSPy compare to existing libraries like LangChain and LlamaIndex in terms of prompt engineering and modularity?" # [should use knowledge base]
python main.py -q "Which teams qualified for round-16 in recent fifa club worldcup?"``` # [should use web search]

# 3. Run tests
pytest
```

## âœ… Python Version

This project uses and expects **Python 3.12.8**.

## ğŸ“ˆ Example Outputs

Want to see how this system responds to user queries?

Check out:  
[`examples/output_samples.md`](examples/output_samples.md)

This includes responses from gemini-2.0-flash only:
- Google Gemini `gemini-2.0-flash` (cloud-based via Free API Key)

## âš™ï¸ LLM Compatibility Observations (CrewAI Agents)

### âœ… Working as Expected
- **Google Gemini 2.0 Flash**
  - âœ… Works perfectly with `CrewAI` agents using `crewai.LLM`, both:
    - When called **independently**:  
      ```python
      llm.call("Test message")
      ```
    - When used **inside CrewAI agents**:
      ```python
      Agent(..., llm=llm, ...)
      ```

### âš ï¸ Issues Observed
- **Local models via Ollama** (e.g., `qwen2.5`, `deepseek-coder`)
  - âœ… Works when called **independently** using `crewai.LLM`:
    ```python
    llm.call("Test message")
    ```
  - âŒ **Fails when used inside CrewAI agents**:
    - Error encountered:
      ```
      LLM Failed Error
      ```
    - Indicates potential issues with local model integration in agent context (e.g., serialization, networking, tool wrapping or api integration etc).


